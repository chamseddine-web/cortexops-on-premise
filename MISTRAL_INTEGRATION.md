# ü§ñ Int√©gration Mistral AI pour CortexOps

## Vue d'ensemble

CortexOps supporte maintenant plusieurs mod√®les AI pour la g√©n√©ration de playbooks Ansible, avec **Mistral AI** comme provider recommand√© pour les cas d'usage DevOps.

## üéØ Mod√®les Mistral recommand√©s

### 1. **mistral-large-latest** - G√©n√©ration Premium
- **Usage** : G√©n√©ration de playbooks complexes, projets production
- **Avantages** : Structure parfaite, compr√©hension approfondie du contexte
- **Co√ªt** : ~$0.008 / 1K tokens
- **Quand l'utiliser** :
  - Playbooks multi-environnements complexes
  - Int√©grations CI/CD avanc√©es
  - Blueprints enterprise avec s√©curit√© renforc√©e
  - Projets critiques en production

### 2. **mistral-small-latest** - Audit & Corrections
- **Usage** : Validation, audit de s√©curit√©, corrections YAML
- **Avantages** : Tr√®s rapide, pr√©cis pour les t√¢ches cibl√©es
- **Co√ªt** : ~$0.002 / 1K tokens (4√ó moins cher)
- **Quand l'utiliser** :
  - Audit de playbooks existants
  - Correction d'erreurs YAML
  - Validation de syntaxe Ansible
  - Suggestions d'optimisation

### 3. **open-mistral-nemo** - Usage quotidien
- **Usage** : Prototypage, tests, g√©n√©ration standard
- **Avantages** : Excellent rapport qualit√©/prix
- **Co√ªt** : ~$0.002 / 1K tokens
- **Quand l'utiliser** :
  - D√©veloppement et prototypage
  - Tests et exp√©rimentation
  - Playbooks simples √† moyens
  - Budget limit√©

### 4. **mistral:7b via Ollama** - Mode Offline
- **Usage** : On-premise, sans connexion, donn√©es sensibles
- **Avantages** : Gratuit, priv√©, fonctionne offline
- **Co√ªt** : Aucun (n√©cessite ressources locales)
- **Quand l'utiliser** :
  - Environnements on-premise stricts
  - Travail sans connexion internet
  - Donn√©es hautement confidentielles
  - √âviter les co√ªts API

## üöÄ Installation et Configuration

### Option 1 : Mistral Cloud API (Recommand√©)

1. **Obtenir une cl√© API Mistral** :
   ```bash
   # Cr√©ez un compte sur https://console.mistral.ai/
   # Puis g√©n√©rez une cl√© API
   ```

2. **Configurer la cl√© dans .env** :
   ```env
   VITE_MISTRAL_API_KEY=your-mistral-api-key-here
   ```

3. **Tester la connexion** :
   ```bash
   npm run dev
   # Dans l'interface, allez dans Param√®tres ‚Üí Mod√®les AI
   # Cliquez sur "Tester connexions"
   ```

### Option 2 : Ollama Local (Mode Offline)

1. **Installer Ollama** :
   ```bash
   # Linux / macOS
   curl -fsSL https://ollama.com/install.sh | sh

   # Windows (via WSL ou installateur)
   # T√©l√©chargez depuis https://ollama.com/download
   ```

2. **T√©l√©charger Mistral 7B** :
   ```bash
   ollama pull mistral:7b
   ```

3. **D√©marrer Ollama** :
   ```bash
   ollama serve
   ```

4. **Configurer l'endpoint** :
   ```env
   VITE_OLLAMA_ENDPOINT=http://localhost:11434
   ```

## üí° Exemples d'utilisation

### Exemple 1 : G√©n√©ration avec Mistral Large

```typescript
import { useAIModel } from '../hooks/useAIModel';

function PlaybookGenerator() {
  const { generate, selectedModel } = useAIModel({
    defaultModel: 'mistral-large'
  });

  const generatePlaybook = async () => {
    const response = await generate(
      'Cr√©er un playbook pour d√©ployer une stack LAMP s√©curis√©e',
      'Tu es un expert Ansible. G√©n√®re des playbooks professionnels avec best practices.'
    );

    console.log('Playbook g√©n√©r√©:', response.content);
    console.log('Co√ªt:', response.cost);
  };
}
```

### Exemple 2 : Audit avec Mistral Small

```typescript
const { generate } = useAIModel({ defaultModel: 'mistral-small' });

const auditPlaybook = async (playbook: string) => {
  const response = await generate(
    `Audite ce playbook et identifie les probl√®mes de s√©curit√©:\n\n${playbook}`,
    'Tu es un auditeur de s√©curit√© Ansible. Identifie les vuln√©rabilit√©s.'
  );

  return response.content;
};
```

### Exemple 3 : Mode Offline avec Ollama

```typescript
const { generate } = useAIModel({ defaultModel: 'mistral-7b-local' });

const generateOffline = async () => {
  try {
    const response = await generate(
      'Cr√©er un playbook basique pour installer nginx'
    );

    console.log('G√©n√©r√© offline:', response.content);
  } catch (error) {
    console.error('Ollama non disponible:', error);
  }
};
```

## üîß Int√©gration dans les composants

### Utiliser le s√©lecteur de mod√®le

```tsx
import { AIModelSelector } from '../components/AIModelSelector';
import { useAIModel } from '../hooks/useAIModel';

function MyComponent() {
  const { selectedModel, setSelectedModel, generate } = useAIModel();

  return (
    <div>
      <AIModelSelector
        selectedModel={selectedModel}
        onModelChange={setSelectedModel}
        estimatedTokens={2000}
      />

      <button onClick={() => generate('Mon prompt')}>
        G√©n√©rer avec {selectedModel}
      </button>
    </div>
  );
}
```

## üìä Comparaison des mod√®les

| Mod√®le | Co√ªt/1K tokens | Vitesse | Qualit√© | Offline | Use Case Principal |
|--------|----------------|---------|---------|---------|-------------------|
| mistral-large-latest | $0.008 | Moyenne | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå | Production complexe |
| mistral-small-latest | $0.002 | Rapide | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå | Audit & corrections |
| open-mistral-nemo | $0.002 | Rapide | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå | Usage quotidien |
| mistral:7b (Ollama) | Gratuit | Moyenne | ‚≠ê‚≠ê‚≠ê | ‚úÖ | On-premise / offline |

## üéì Best Practices

### 1. Choisir le bon mod√®le selon le contexte

```typescript
function getRecommendedModel(task: string) {
  if (task.includes('audit') || task.includes('correct')) {
    return 'mistral-small';
  }
  if (task.includes('complex') || task.includes('enterprise')) {
    return 'mistral-large';
  }
  return 'mistral-nemo'; // Default
}
```

### 2. G√©rer les erreurs API

```typescript
const { generate, error } = useAIModel({
  onError: (err) => {
    if (err.message.includes('API key')) {
      console.error('Cl√© API invalide ou manquante');
    } else if (err.message.includes('quota')) {
      console.error('Quota API d√©pass√©');
    }
  }
});
```

### 3. Optimiser les co√ªts

```typescript
// Pour les t√¢ches simples, utilisez le mod√®le le plus √©conomique
const simpleTask = async (prompt: string) => {
  const { generate } = useAIModel({ defaultModel: 'mistral-nemo' });
  return await generate(prompt, undefined, {
    maxTokens: 500 // Limiter les tokens
  });
};
```

### 4. Cache et r√©utilisation

```typescript
// Mettez en cache les r√©sultats pour √©viter les appels redondants
const cache = new Map<string, AIGenerationResponse>();

const generateWithCache = async (prompt: string) => {
  if (cache.has(prompt)) {
    return cache.get(prompt);
  }

  const response = await generate(prompt);
  cache.set(prompt, response);
  return response;
};
```

## üîí S√©curit√©

### Variables d'environnement
- ‚úÖ Stockez les cl√©s API dans `.env` (jamais en dur dans le code)
- ‚úÖ Utilisez `.env.local` pour le d√©veloppement
- ‚úÖ Ne committez JAMAIS les fichiers `.env` dans Git

### Mode On-Premise
- Pour les donn√©es sensibles, utilisez **Ollama** en local
- Aucune donn√©e n'est envoy√©e √† des services cloud
- Contr√¥le total sur l'infrastructure

## üìà Monitoring et Analytics

### Suivre les co√ªts

```typescript
import { calculateEstimatedCost } from '../lib/aiModelConfig';

const estimatedCost = calculateEstimatedCost('mistral-large', 2000);
console.log(`Co√ªt estim√©: $${estimatedCost.toFixed(4)}`);
```

### Tracker les performances

```typescript
const { lastResponse } = useAIModel();

if (lastResponse) {
  console.log('Temps de traitement:', lastResponse.processingTime, 'ms');
  console.log('Tokens utilis√©s:', lastResponse.tokensUsed);
  console.log('Co√ªt r√©el:', lastResponse.cost);
}
```

## üÜò Troubleshooting

### Probl√®me : "Cl√© API Mistral manquante"
**Solution** : V√©rifiez que `VITE_MISTRAL_API_KEY` est d√©fini dans `.env`

### Probl√®me : "Ollama error: ECONNREFUSED"
**Solution** : D√©marrez Ollama avec `ollama serve` ou v√©rifiez l'endpoint

### Probl√®me : "Quota exceeded"
**Solution** : V√©rifiez votre compte Mistral ou basculez sur Ollama local

### Probl√®me : G√©n√©ration trop lente
**Solution** : Utilisez `mistral-small` ou `mistral-nemo` au lieu de `mistral-large`

## üìö Ressources

- [Documentation Mistral AI](https://docs.mistral.ai/)
- [Console Mistral](https://console.mistral.ai/)
- [Ollama Documentation](https://github.com/ollama/ollama)
- [Pricing Mistral](https://mistral.ai/pricing/)

## ü§ù Contribution

Pour ajouter un nouveau mod√®le, √©ditez `src/lib/aiModelConfig.ts` :

```typescript
export const AI_MODELS: Record<string, AIModelConfig> = {
  'my-new-model': {
    provider: 'mistral',
    model: 'my-model-name',
    description: 'Description du mod√®le',
    useCases: ['Use case 1', 'Use case 2'],
    costPerToken: 0.000001,
    maxTokens: 16000,
    temperature: 0.7,
    requiresApiKey: true
  }
};
```
